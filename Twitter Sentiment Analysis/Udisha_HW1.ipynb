{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading all txt files in jupyter\n",
    "trump = open('/Users/udisha/Downloads/Fall21/2507_Data_Analytics/HW1/Trump.txt','r')\n",
    "positive = open('/Users/udisha/Downloads/Fall21/2507_Data_Analytics/HW1/positive.txt','r')\n",
    "negative = open('/Users/udisha/Downloads/Fall21/2507_Data_Analytics/HW1/negative-words.txt','r')\n",
    "stop = open('/Users/udisha/Downloads/Fall21/2507_Data_Analytics/HW1/stopwords.txt','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading all files into list\n",
    "trump_all = trump.read()\n",
    "positive_all = positive.read()\n",
    "negative_all = negative.read()\n",
    "stop_all = stop.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list with each word as an element\n",
    "trump_list = trump_all.split()\n",
    "positive_list = positive_all.split()\n",
    "#Removing the word 'Trump' from the list of positive words\n",
    "positive_list_woTrump = positive_list.remove('trump')\n",
    "negative_list = negative_all.split()\n",
    "stop_list = stop_all.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Cleaning Trump file by removing special characters, spaces, random patterns found and converting them to lower case\n",
    "import re\n",
    "trump_clean = []\n",
    "for each in trump_list:\n",
    "    each = each.lower()\n",
    "    each = re.sub(r'[^\\w]',' ', each).strip()\n",
    "    each = re.sub(r'x80',' ', each)\n",
    "    each = re.sub(r'xe2',' ', each)\n",
    "    each = re.sub(r'xa6',' ', each)\n",
    "    each = each.strip()\n",
    "    trump_clean.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flattening the nested list created because of presence of random characters between 2 words instead of space\n",
    "for a in range(len(trump_clean)):\n",
    "    trump_clean[a] = trump_clean[a].split()\n",
    "\n",
    "trump_clean = [item for sublist in trump_clean for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the presence of items of Trump file in positive, negative, stop. Everything else goes into 'other'\n",
    "trump_positive_count = 0\n",
    "trump_negative_count = 0\n",
    "trump_stop_count = 0\n",
    "trump_others = 0\n",
    "positiveWords = []\n",
    "negativeWords = []\n",
    "for each in trump_clean:\n",
    "    if each in positive_list:\n",
    "        positiveWords.append(each)\n",
    "        trump_positive_count+=1\n",
    "    elif each in negative_list:\n",
    "        negativeWords.append(each)\n",
    "        trump_negative_count+=1\n",
    "    elif each in stop_list:\n",
    "        trump_stop_count+=1\n",
    "    else:\n",
    "        trump_others+= 1\n",
    "total_count = trump_positive_count + trump_negative_count + trump_stop_count + trump_others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of positive words found are:  999\n",
      "Total number of negative words found are:  1886\n",
      "Total number of stop words found are:  29117\n",
      "Total number of other words found are:  33656\n",
      "Ratio of positive words:  0.02\n",
      "Ratio of negative words:  0.03\n",
      "Ratio of stop words:  0.44\n",
      "Ratio of other words:  0.51\n",
      "Sum of positive and negative words :  2885\n",
      "The general sentiment of the tweets is strongly negative\n"
     ]
    }
   ],
   "source": [
    "#Printing the number of words found for each group\n",
    "print (\"Total number of positive words found are: \" , trump_positive_count)\n",
    "print (\"Total number of negative words found are: \" , trump_negative_count)\n",
    "print (\"Total number of stop words found are: \" , trump_stop_count)\n",
    "print (\"Total number of other words found are: \" , trump_others)\n",
    "print (\"Ratio of positive words: \" , format(trump_positive_count/total_count,\"0.2f\"))\n",
    "print (\"Ratio of negative words: \" , format(trump_negative_count/total_count, \"0.2f\"))\n",
    "print (\"Ratio of stop words: \" , format(trump_stop_count/total_count, \"0.2f\"))\n",
    "print (\"Ratio of other words: \" , format(trump_others/total_count, \"0.2f\"))\n",
    "print (\"Sum of positive and negative words : \", trump_negative_count+trump_positive_count)\n",
    "print (\"The general sentiment of the tweets is strongly negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2: Study on tweets about Suarez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Suarez tweets txt files in jupyter\n",
    "suarez = open('/Users/udisha/Downloads/Fall21/2507_Data_Analytics/HW1/Suarez.txt','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Suarez file\n",
    "suarez_all = suarez.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating list after splitting the file using spaces\n",
    "suarez_list = suarez_all.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the file - Pattern found : '\\\\' before random words. Hence, used that to add blanks between words\n",
    "suarez_clean = []\n",
    "for each in suarez_list:\n",
    "    each = each.lower()\n",
    "    each = re.sub(r'[^\\w]',' ', each).strip()\n",
    "    each = re.sub(r'x80',' ', each)\n",
    "    each = re.sub(r'xe2',' ', each)\n",
    "    each = re.sub(r'xa6',' ', each)\n",
    "    each = re.sub(r'xc3',' ', each)\n",
    "    each = re.sub(r'x9f',' ', each)\n",
    "    each = re.sub(r'xf0',' ', each)\n",
    "    each = re.sub(r'x99',' ', each)\n",
    "    each = re.sub(r'x9f',' ', each)\n",
    "    each = re.sub(r'xc3',' ', each)\n",
    "    each = re.sub(r'\\\\',' ', each)\n",
    "    each = each.strip()\n",
    "    suarez_clean.append(each)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flattening the nested list \n",
    "for a in range(len(suarez_clean)):\n",
    "    suarez_clean[a] = suarez_clean[a].split()\n",
    "\n",
    "suarez_clean = [item for sublist in suarez_clean for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping words in Suarez into positive, negative, stop and others\n",
    "suarez_positive_count = 0\n",
    "suarez_negative_count = 0\n",
    "suarez_stop_count = 0\n",
    "suarez_others = 0\n",
    "for each in suarez_clean:\n",
    "    if each in positive_list:\n",
    "        suarez_positive_count+=1\n",
    "    elif each in negative_list:\n",
    "        suarez_negative_count+=1\n",
    "    elif each in stop_list:\n",
    "        suarez_stop_count+=1\n",
    "    else:\n",
    "        suarez_others+= 1\n",
    "        \n",
    "total = suarez_positive_count + suarez_negative_count + suarez_stop_count + suarez_others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of positive words found are:  40\n",
      "Total number of negative words found are:  22\n",
      "Total number of stop words found are:  1247\n",
      "Total number of other words found are:  1464\n",
      "Ratio of positive words:  0.01\n",
      "Ratio of negative words:  0.01\n",
      "Ratio of stop words:  0.45\n",
      "Ratio of other words:  0.53\n",
      "Sum of positive and negative words :  62\n",
      "The general sentiment of the tweets is strongly positive\n"
     ]
    }
   ],
   "source": [
    "#Printing the number of words found for each group\n",
    "print (\"Total number of positive words found are: \" , suarez_positive_count)\n",
    "print (\"Total number of negative words found are: \" , suarez_negative_count)\n",
    "print (\"Total number of stop words found are: \" , suarez_stop_count)\n",
    "print (\"Total number of other words found are: \" , suarez_others)\n",
    "print (\"Ratio of positive words: \" , format(suarez_positive_count/total,\"0.2f\"))\n",
    "print (\"Ratio of negative words: \" , format(suarez_negative_count/total, \"0.2f\"))\n",
    "print (\"Ratio of stop words: \" , format(suarez_stop_count/total, \"0.2f\"))\n",
    "print (\"Ratio of other words: \" , format(suarez_others/total, \"0.2f\"))\n",
    "print (\"Sum of positive and negative words : \", suarez_negative_count+suarez_positive_count)\n",
    "print (\"The general sentiment of the tweets is strongly positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
